{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from m5.features import add_special_event_feature, add_snap_feature, add_demand_type_feature\n",
    "from m5.hierarchy import compute_summing_matrix, get_sales_long, get_prices_long, compute_series_weights, get_sales_long_with_nan\n",
    "# from rpy2_models.ts_model import TimeSeriesModel, SExpS, Croston, ESX, OES, ExponentialSmoothing\n",
    "# from rpy2_models import r_models_functions\n",
    "# from rpy2.robjects import NULL\n",
    "# from m5.forecast import forecast_bottom_level, get_rmsse, get_wrmsse\n",
    "\n",
    "from m5.global_model import get_X_and_y\n",
    "from m5.hierarchy import compute_series_weights\n",
    "\n",
    "\n",
    "data = Path('data/')\n",
    "sales = pd.read_csv(data / 'sales_train_validation.csv', dtype={\n",
    "    'item_id':'category', 'dept_id':'category', 'cat_id':'category', 'store_id':'category', 'state_id':'category'\n",
    "})\n",
    "prices = pd.read_csv(data / 'sell_prices.csv', dtype={\n",
    "    'store_id': 'category', 'item_id': 'category'\n",
    "})\n",
    "calendar = pd.read_csv(data / 'calendar.csv', parse_dates=['date'], dtype={\n",
    "    'weekday':'category', 'd':'category', 'event_name':'category', 'event_type_1':'category', 'event_type_2':'category',\n",
    "    'snap_CA':'category', 'snap_TX':'category', 'snap_WI':'category'\n",
    "})\n",
    "\n",
    "d_cols = [c for c in sales.columns if c.startswith('d_')]\n",
    "id_cols = [c for c in sales.columns if c not in d_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## [[https://mk0mcompetitiont8ake.kinstacdn.com/wp-content/uploads/2020/02/M5-Competitors-Guide_Final-1.pdf][Competition rules]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Interesting details\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "-   date ranges are 2011-01-29 to 2016-06-19, 1,941 days (5.4 years)\n",
    "-   In the `calendar.csv` file there are 3 columns for SNAP: for 10 days of the\n",
    "    month, a subsidy is given to some people to buy some food\n",
    "-   For each item, the (average) weekly price is given. The price can change over\n",
    "    time and when it is not available it means that the product was not sold\n",
    "    during that period (!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "-   First the RMSSE metrics is computed for each series\n",
    "    -   This is a modified version of the MASE\n",
    "-   Then a weighted average (WRMSSE) is calculated.\n",
    "    -   The weight is calculated by looking at the last 28 days of the training set\n",
    "        and computing (sum of units sold \\* price)\n",
    "    -   Not only! The WRMSSE is computed by considering errors done at all the\n",
    "        different aggregation levels (12)\n",
    "        -   All levels are equally weighted\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Benchmarks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "-   Statistical benchmarks methods:\n",
    "    -   Croston&rsquo;s method: decompose series into non-zero and inter-demand intervals\n",
    "    -   Temporal aggregation: ADIDA and iMAPA\n",
    "    -   ESX and ARIMAX, exog features:\n",
    "        -   Number of states allowing SNAP purchases (0, 1, 2, 3)\n",
    "        -   Binary for special event\n",
    "-   ML benchmark methods:\n",
    "    -   MLP taking last 14 days as input of each single series, 28 hidden nodes and one output node\n",
    "    -   RF taking last 14 days as input\n",
    "        -   4 features sampled at each split\n",
    "    -   Global MLP, learning from last 14 observations of all series\n",
    "        -   Coefficient of variation of non-zero demands (CV<sup>2</sup>) to distinguish the series\n",
    "        -   Average number of time periods between 2 successive non-zero demands (ADI)\n",
    "    -   Global RF, same approach as GMLP\n",
    "-   ARIMAX and ESX forecast at the topmost level, while global models and local models forecast at the\n",
    "    lowest level (prouct+store)\n",
    "    -   To reconcile forecasts, we calculate historical proportions according the\n",
    "        last 28 days\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Hierarchical time series\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sales_long = get_sales_long(sales)\n",
    "sum_matrix = compute_summing_matrix(sales)\n",
    "prices_long = get_prices_long(prices, calendar, sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sales_long.to_hdf(data / 'sales_long.h5', key='sales_long')\n",
    "pd.read_hdf(data / 'sales_long.h5', key='sales_long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "weights = compute_series_weights(sales, prices, calendar)\n",
    "weights.to_hdf(data / 'weights_all_levels.h5', key='weights_all_levels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Grouped time series\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Hierarchy state > store > cat > dept > item leads to aggregates by:\n",
    "\n",
    "-   Total\n",
    "-   State (3)\n",
    "-   Store (10)\n",
    "-   Store + category (30)\n",
    "-   Store + department (70)\n",
    "-   Item + Store (30490)\n",
    "\n",
    "Hierarchy state > category > dept > store:\n",
    "\n",
    "-   State (3)\n",
    "-   State + category (9)\n",
    "-   State + department (21)\n",
    "-   Store + department (70)\n",
    "\n",
    "Hierarchy category > department > state > store > item:\n",
    "\n",
    "-   Category (3)\n",
    "-   Department (7)\n",
    "-   Department + State (21)\n",
    "-   Department + Store (70)\n",
    "-   Item + Store (30490)\n",
    "\n",
    "Each product aggregated by:\n",
    "\n",
    "-   Total (all stores) (3049)\n",
    "-   State (9147)\n",
    "-   Store (30490)\n",
    "\n",
    "    graph TD\n",
    "    Total-->|3|State\n",
    "    State-->|10|Store\n",
    "    State-->|9|State+Category\n",
    "    State+Category-->|30|Store+Category\n",
    "    Store-->|30|Store+Category\n",
    "    Store+Category-->|70|Store+Department\n",
    "    Store+Department-->|30490|Store+Item\n",
    "    \n",
    "    Total-->|3|Category\n",
    "    Category-->|7|Department\n",
    "    Department-->|21|Department+State\n",
    "    Department+State-->|70|Store+Department\n",
    "    \n",
    "    Total-->|3049|Item\n",
    "    Item-->|9147|Item+State\n",
    "    Item+State-->|30490|Store+Item\n",
    "\n",
    "![img](/Users/luca/git/experiments/data/kaggle_hierarchy.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### sales_long\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "all_names = ['level', 'state_id', 'store_id', 'cat_id', 'dept_id', 'item_id']\n",
    "all_groups = {'total': ['d'],\n",
    "        'state': ['d', 'state_id'], 'cat': ['d', 'cat_id'], 'item': ['d', 'item_id'],\n",
    "        'store': ['d', 'state_id', 'store_id'], 'state/cat':['d', 'state_id', 'cat_id'], 'dept': ['d', 'cat_id', 'dept_id'], 'item/state': ['d', 'item_id', 'state_id'],\n",
    "        'store/cat': ['d', 'state_id', 'store_id', 'cat_id'], 'dept/state': ['d', 'cat_id', 'dept_id', 'state_id'],\n",
    "        'store/dept': ['d', 'state_id', 'store_id', 'cat_id', 'dept_id']}\n",
    "\n",
    "\n",
    "def get_sales_long(sales: pd.DataFrame, groups: Dict[str, List[str]] = all_groups, include_bottom_level=True, last_28_days=False):\n",
    "    d_cols = [c for c in sales.columns if c.startswith('d_')]\n",
    "    if last_28_days:\n",
    "        d_cols = d_cols[-28:]\n",
    "\n",
    "    if groups:\n",
    "        sales_melted = sales.melt(id_vars=all_names[1:], value_vars=d_cols, var_name='d', value_name='sales')\n",
    "\n",
    "    all_sales_grouped = []\n",
    "    for level, group in groups.items():\n",
    "        sales_grouped = sales_melted.groupby(group, as_index=False).sum()\n",
    "\n",
    "        missing = [c for c in all_names if c not in group]\n",
    "        for m in missing:\n",
    "            sales_grouped.loc[:, m] = 'all'\n",
    "        sales_grouped.loc[:, 'level'] = level\n",
    "\n",
    "        sales_grouped = sales_grouped.set_index(['d'] + all_names).unstack(all_names)\n",
    "        sales_grouped.columns = sales_grouped.columns.droplevel(0)\n",
    "        all_sales_grouped.append(sales_grouped)\n",
    "\n",
    "    if include_bottom_level:\n",
    "        sales_grouped = sales[all_names[1:] + d_cols]\n",
    "        sales_grouped.loc[:, 'level'] = 'item/store'\n",
    "        sales_grouped = sales_grouped.set_index(all_names).T\n",
    "        all_sales_grouped.insert(0, sales_grouped)\n",
    "\n",
    "    all_sales_grouped = pd.concat(all_sales_grouped, axis=1)\n",
    "\n",
    "    return all_sales_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# table = pa.Table.from_pandas(df_test)\n",
    "# pq.write_table(table, 'test.parquet')\n",
    "table = pq.read_table('/Users/luca/git/experiments/data/sales_long.parquet', use_pandas_metadata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Summing matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[https://otexts.com/fpp2/hts.html](https://otexts.com/fpp2/hts.html)\n",
    "Matrix having as number of columns the number of bottom series and as number of\n",
    "rows the total number of time series.\n",
    "The bottom part is a square matrix with 1 on the diagonal.\n",
    "\n",
    "$$ \\bm{y}_t = \\bm{S b}_t $$\n",
    "\n",
    "At the bottom level there are 70 series describing the store+department level.\n",
    "There are 10 stores, each of which has 7 departments. Each of these series is represented in\n",
    "the matrix by a row having one non-zero entry.\n",
    "\n",
    "The level above contains 30 series for the store+category series. Again, 10\n",
    "stores, 3 categories. We need to create 30 rows in the matrix. Each row\n",
    "describes which of the bottom series aggregates to form that store+category entry.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def compute_summing_matrix(sales: pd.DataFrame, groups=all_groups.copy()):\n",
    "    d_cols = [c for c in sales.columns if c.startswith('d_')]\n",
    "\n",
    "    sales_grouped = sales.groupby(all_names[1:])[d_cols].sum()\n",
    "    bottom_level = sales_grouped.index\n",
    "    sales_grouped['bottom_level_index'] = sales_grouped.reset_index().index\n",
    "\n",
    "    top_row = pd.DataFrame(data=np.ones((1,sales_grouped.shape[0])), index=['total'], columns=bottom_level)\n",
    "    groups.pop('total', None)\n",
    "\n",
    "    bottom_submatrix = pd.DataFrame(data=np.identity(sales_grouped.shape[0]), index=bottom_level, columns=bottom_level)\n",
    "\n",
    "    central_submatrices = []\n",
    "    for _, group in groups.items():\n",
    "        sales_level = sales_grouped.groupby(group[1:])['bottom_level_index'].agg(list)\n",
    "\n",
    "        level_rows = []\n",
    "        for _, group in sales_level.iteritems():\n",
    "            row = np.zeros(sales_grouped.shape[0])\n",
    "            row[group] = 1\n",
    "            level_rows.append(row)\n",
    "        level_submatrix = pd.DataFrame(data=np.vstack(level_rows), index=sales_level.index, columns=bottom_level)\n",
    "        central_submatrices.insert(0, level_submatrix)\n",
    "\n",
    "    sum_matrix = pd.concat([top_row] + central_submatrices + [bottom_submatrix], sort=False)\n",
    "\n",
    "    return sum_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sum_mat = compute_summing_matrix(sales)\n",
    "sum_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "groups = {'store': ['d', 'state_id', 'store_id'], 'state/cat':['d', 'state_id', 'cat_id'], 'dept': ['d', 'cat_id', 'dept_id'], 'item/state': ['d', 'item_id', 'state_id']}\n",
    "sum_mat = compute_summing_matrix(sales, groups)\n",
    "sum_mat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Prices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_prices_long(prices: pd.DataFrame, calendar: pd.DataFrame, sales: pd.DataFrame, groups: Dict[str, List[str]] = all_groups, include_bottom_level=True, last_28_days=True):\n",
    "    prices_d = prices.merge(calendar[['wm_yr_wk', 'd']]).drop(columns=['wm_yr_wk'])\n",
    "    d_cols = [c for c in sales.columns if c.startswith('d_')]\n",
    "    if last_28_days:\n",
    "        prices_d = prices_d[prices_d['d'].isin(d_cols[-28:])]\n",
    "    prices_d = sales[all_names[1:]].merge(prices_d)\n",
    "    prices_d.loc[:, 'level'] = 'item/store'\n",
    "\n",
    "    all_prices_h = []\n",
    "    for level, group in groups.items():\n",
    "        prices_h = prices_d.groupby(group, as_index=False).sum()\n",
    "\n",
    "        missing = [c for c in all_names if c not in group]\n",
    "        for m in missing:\n",
    "            prices_h.loc[:, m] = 'all'\n",
    "        prices_h.loc[:, 'level'] = level\n",
    "\n",
    "        prices_h = prices_h.set_index(['d'] + all_names).unstack(all_names)\n",
    "        prices_h.columns = prices_h.columns.droplevel(0)\n",
    "        all_prices_h.append(prices_h)\n",
    "\n",
    "    if include_bottom_level:\n",
    "        prices_h = prices_d.set_index(['d'] + all_names).unstack(all_names)\n",
    "        prices_h.columns = prices_h.columns.droplevel(0)\n",
    "        all_prices_h.insert(0, prices_h)\n",
    "\n",
    "    all_prices_h = pd.concat(all_prices_h, axis=1)\n",
    "    return all_prices_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For each series find its dollar value by multiplying its sales in the last 28 days by\n",
    "the price it had.\n",
    "Then, divide it by the total dollar value of the level.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def compute_series_weights(sales: pd.DataFrame, prices: pd.DataFrame, calendar: pd.DataFrame, groups: Dict[str, List[str]] = all_groups, include_bottom_level=True):\n",
    "    prices_long = get_prices_long(prices, calendar, sales, groups=groups, include_bottom_level=include_bottom_level, last_28_days=True)\n",
    "    sales_long = get_sales_long(sales, groups=groups, include_bottom_level=include_bottom_level, last_28_days=True)\n",
    "\n",
    "    assert all(sales_long.columns == prices_long.columns)\n",
    "    assert all(sales_long.index == prices_long.index)\n",
    "\n",
    "    dollar_value_by_series = (prices_long * sales_long).sum(axis=0).to_frame(name='series_dollar_value')\n",
    "    dollar_value_by_level = (prices_long * sales_long).sum(axis=0).sum(level='level').to_frame(name='level_dollar_value')\n",
    "    dollar_value = dollar_value_by_series.merge(dollar_value_by_level, left_index=True, right_index=True)\n",
    "    dollar_value.loc[:, 'series_dollar_value_scaled'] = dollar_value['series_dollar_value'] / dollar_value['level_dollar_value']\n",
    "    assert dollar_value['series_dollar_value_scaled'].sum(level='level').all() == 1\n",
    "\n",
    "    dollar_value.loc[:, 'series_weight'] = dollar_value['series_dollar_value_scaled'] / (1 + len(groups))\n",
    "    return dollar_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "groups = {'store': ['d', 'state_id', 'store_id'], 'state/cat':['d', 'state_id', 'cat_id'], 'dept': ['d', 'cat_id', 'dept_id'], 'item/state': ['d', 'item_id', 'state_id']}\n",
    "weights_df = compute_series_weights(sales_long, prices_long, groups)\n",
    "weights_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Identify when a product was not available\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_sales_long_with_nan(sales, prices, calendar, groups={}):\n",
    "    prices_long = get_prices_long(prices, calendar, sales, groups=groups, last_28_days=False)\n",
    "    sales_long = get_sales_long(sales, groups=groups)\n",
    "    assert all(sales_long.columns == prices_long.columns)\n",
    "\n",
    "    prices_long = prices_long.loc[sales_long.index]\n",
    "    assert all(sales_long.index == prices_long.index)\n",
    "\n",
    "    sales_long[prices_long.isna()] = pd.NA\n",
    "    return sales_long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## snap features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def add_snap_feature(calendar: pd.DataFrame):\n",
    "    snap_cols = [col for col in calendar.columns if col.startswith('snap_')]\n",
    "    calendar['snap'] = calendar[snap_cols].sum(axis=1)\n",
    "    return calendar\n",
    "\n",
    "def add_special_event_feature(calendar: pd.DataFrame):\n",
    "    calendar['special_event'] = 0\n",
    "    calendar.loc[calendar['event_type_1'].notna(), 'special_event'] = 1\n",
    "    return calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Bottom-level models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sales = add_demand_type_feature(sales)\n",
    "# sales[sales.type == 'smooth'][d_cols].iloc[300, -52:].plot()\n",
    "one = sales[sales.type == 'smooth'][d_cols].iloc[300:301, :].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "y_pred_long = forecast_bottom_level(sales, prices, calendar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Univariate itermittent models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[https://kourentzes.com/forecasting/wp-content/uploads/2014/05/Petropoulos-Kourentzes-2014-Forecast-Combinations-for-Intermittent-Demand.pdf](https://kourentzes.com/forecasting/wp-content/uploads/2014/05/Petropoulos-Kourentzes-2014-Forecast-Combinations-for-Intermittent-Demand.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### R functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    library(zoo)\n",
    "    \n",
    "    intervals <- function(x){\n",
    "      y<-c()\n",
    "      k<-1\n",
    "      counter<-0\n",
    "      for (tmp in (1:length(x))){\n",
    "        if(x[tmp]==0){\n",
    "          counter<-counter+1\n",
    "        }else{\n",
    "          k<-k+1\n",
    "          y[k]<-counter\n",
    "          counter<-1\n",
    "        }\n",
    "      }\n",
    "      y<-y[y>0]\n",
    "      y[is.na(y)]<-1\n",
    "      y\n",
    "    }\n",
    "    demand <- function(x){\n",
    "      y<-x[x!=0]\n",
    "      y\n",
    "    }\n",
    "    SexpS <- function(x, h){\n",
    "      a <- optim(c(0), SES, x=x, h=1, job=\"train\", lower = 0.1, upper = 0.3, method = \"L-BFGS-B\")$par\n",
    "      y <- SES(a=a, x=x, h=1, job=\"forecast\")$mean\n",
    "      forecast <- rep(as.numeric(y), h)\n",
    "      return(forecast)\n",
    "    }\n",
    "    SES <- function(a, x, h, job){\n",
    "      y <- c()\n",
    "      y[1] <- x[1] #initialization\n",
    "    \n",
    "      for (t in 1:(length(x))){\n",
    "        y[t+1] <- a*x[t]+(1-a)*y[t]\n",
    "      }\n",
    "    \n",
    "      fitted <- head(y,(length(y)-1))\n",
    "      forecast <- rep(tail(y,1),h)\n",
    "      if (job==\"train\"){\n",
    "        return(mean((fitted - x)^2))\n",
    "      }else if (job==\"fit\"){\n",
    "        return(fitted)\n",
    "      }else{\n",
    "        return(list(fitted=fitted,mean=forecast))\n",
    "      }\n",
    "    }\n",
    "    Croston <- function(x, h, type){\n",
    "      if (type==\"classic\"){\n",
    "        mult <- 1\n",
    "        a1 = a2 <- 0.1\n",
    "      }else if (type==\"optimized\"){\n",
    "        mult <- 1\n",
    "        a1 <- optim(c(0), SES, x=demand(x), h=1, job=\"train\", lower = 0.1, upper = 0.3, method = \"L-BFGS-B\")$par\n",
    "        a2 <- optim(c(0), SES, x=intervals(x), h=1, job=\"train\", lower = 0.1, upper = 0.3, method = \"L-BFGS-B\")$par\n",
    "      }else if (type==\"sba\"){\n",
    "        mult <- 0.95\n",
    "        a1 = a2 <- 0.1\n",
    "      }\n",
    "      yd <- SES(a=a1, x=demand(x), h=1, job=\"forecast\")$mean\n",
    "      yi <- SES(a=a2, x=intervals(x), h=1, job=\"forecast\")$mean\n",
    "      forecast <- rep(as.numeric(yd/yi), h)*mult\n",
    "      return(forecast)\n",
    "    }\n",
    "    TSB <- function(x, h){\n",
    "      n <- length(x)\n",
    "      p <- as.numeric(x != 0)\n",
    "      z <- x[x != 0]\n",
    "    \n",
    "      a <- c(0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5, 0.8)\n",
    "      b <- c(0.01,0.02,0.03,0.05,0.1,0.2,0.3)\n",
    "      MSE <- c() ; forecast <- NULL\n",
    "      for (atemp in a){\n",
    "        for (btemp in b){\n",
    "          zfit <- vector(\"numeric\", length(x))\n",
    "          pfit <- vector(\"numeric\", length(x))\n",
    "          zfit[1] <- z[1] ; pfit[1] <- p[1]\n",
    "    \n",
    "          for (i in 2:n) {\n",
    "            pfit[i] <- pfit[i-1] + atemp*(p[i]-pfit[i-1])\n",
    "            if (p[i] == 0) {\n",
    "              zfit[i] <- zfit[i-1]\n",
    "            }else {\n",
    "              zfit[i] <- zfit[i-1] + btemp*(x[i]-zfit[i-1])\n",
    "            }\n",
    "          }\n",
    "          yfit <- pfit * zfit\n",
    "          forecast[length(forecast)+1] <- list(rep(yfit[n], h))\n",
    "          yfit <- c(NA, head(yfit, n-1))\n",
    "          MSE <- c(MSE, mean((yfit-x)^2, na.rm = T) )\n",
    "        }\n",
    "      }\n",
    "      return(forecast[[which.min(MSE)]])\n",
    "    }\n",
    "    ADIDA <- function(x, h){\n",
    "      al <- round(mean(intervals(x)),0) #mean inter-demand interval\n",
    "      #Aggregated series (AS)\n",
    "      AS <- as.numeric(na.omit(as.numeric(rollapply(tail(x, (length(x) %/% al)*al), al, FUN=sum, by = al))))\n",
    "      forecast <- rep(SexpS(AS, 1)/al, h)\n",
    "      return(forecast)\n",
    "    }\n",
    "    iMAPA <- function(x, h){\n",
    "      mal <- round(mean(intervals(x)),0)\n",
    "      frc <- NULL\n",
    "      for (al in 1:mal){\n",
    "        frc <- rbind(frc, rep(SexpS(as.numeric(na.omit(as.numeric(rollapply(tail(x, (length(x) %/% al)*al), al, FUN=sum, by = al)))), 1)/al, h))\n",
    "      }\n",
    "      forecast <- colMeans(frc)\n",
    "    return(forecast)\n",
    "    }\n",
    "\n",
    "    x <- c(1,0,3,4,5,23,3,4)\n",
    "    iMAPA(x, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### rpy2 functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "<<rpy2_r_functions>>\n",
    "\n",
    "r(\n",
    "\"\"\"\n",
    "    library(zoo)\n",
    "\n",
    "\n",
    "    intervals <- function(x){\n",
    "    y<-c()\n",
    "    k<-1\n",
    "    counter<-0\n",
    "    for (tmp in (1:length(x))){\n",
    "        if(x[tmp]==0){\n",
    "        counter<-counter+1\n",
    "        }else{\n",
    "        k<-k+1\n",
    "        y[k]<-counter\n",
    "        counter<-1\n",
    "        }\n",
    "    }\n",
    "    y<-y[y>0]\n",
    "    y[is.na(y)]<-1\n",
    "    y\n",
    "    }\n",
    "\n",
    "    demand <- function(x){\n",
    "    y<-x[x!=0]\n",
    "    y\n",
    "    }\n",
    "\n",
    "    SES <- function(a, x, h, job){\n",
    "    y <- c()\n",
    "    y[1] <- x[1] #initialization\n",
    "\n",
    "    for (t in 1:(length(x))){\n",
    "        y[t+1] <- a*x[t]+(1-a)*y[t]\n",
    "    }\n",
    "\n",
    "    fitted <- head(y,(length(y)-1))\n",
    "    forecast <- rep(tail(y,1),h)\n",
    "    if (job==\"train\"){\n",
    "        return(mean((fitted - x)^2))\n",
    "    }else if (job==\"fit\"){\n",
    "        return(fitted)\n",
    "    }else{\n",
    "        return(list(fitted=fitted,mean=forecast))\n",
    "    }\n",
    "    }\n",
    "\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "sexps = r(\"\"\"\n",
    "function(x, h){\n",
    "  a <- optim(c(0), SES, x=x, h=1, job=\"train\", lower = 0.1, upper = 0.3, method = \"L-BFGS-B\")$par\n",
    "  y <- SES(a=a, x=x, h=1, job=\"forecast\")$mean\n",
    "  forecast <- rep(as.numeric(y), h)\n",
    "  return(forecast)\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "croston = r(\"\"\"\n",
    "function(x, h, type){\n",
    "    if (type==\"classic\"){\n",
    "        mult <- 1\n",
    "        a1 = a2 <- 0.1\n",
    "    }else if (type==\"optimized\"){\n",
    "        mult <- 1\n",
    "        a1 <- optim(c(0), SES, x=demand(x), h=1, job=\"train\", lower = 0.1, upper = 0.3, method = \"L-BFGS-B\")$par\n",
    "        a2 <- optim(c(0), SES, x=intervals(x), h=1, job=\"train\", lower = 0.1, upper = 0.3, method = \"L-BFGS-B\")$par\n",
    "    }else if (type==\"sba\"){\n",
    "        mult <- 0.95\n",
    "        a1 = a2 <- 0.1\n",
    "    }\n",
    "    yd <- SES(a=a1, x=demand(x), h=1, job=\"forecast\")$mean\n",
    "    yi <- SES(a=a2, x=intervals(x), h=1, job=\"forecast\")$mean\n",
    "    forecast <- rep(as.numeric(yd/yi), h)*mult\n",
    "    return(forecast)\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "tsb = r(\"\"\"\n",
    "function(x, h){\n",
    "  n <- length(x)\n",
    "  p <- as.numeric(x != 0)\n",
    "  z <- x[x != 0]\n",
    "\n",
    "  a <- c(0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5, 0.8)\n",
    "  b <- c(0.01,0.02,0.03,0.05,0.1,0.2,0.3)\n",
    "  MSE <- c() ; forecast <- NULL\n",
    "  for (atemp in a){\n",
    "    for (btemp in b){\n",
    "      zfit <- vector(\"numeric\", length(x))\n",
    "      pfit <- vector(\"numeric\", length(x))\n",
    "      zfit[1] <- z[1] ; pfit[1] <- p[1]\n",
    "\n",
    "      for (i in 2:n) {\n",
    "        pfit[i] <- pfit[i-1] + atemp*(p[i]-pfit[i-1])\n",
    "        if (p[i] == 0) {\n",
    "          zfit[i] <- zfit[i-1]\n",
    "        }else {\n",
    "          zfit[i] <- zfit[i-1] + btemp*(x[i]-zfit[i-1])\n",
    "        }\n",
    "      }\n",
    "      yfit <- pfit * zfit\n",
    "      forecast[length(forecast)+1] <- list(rep(yfit[n], h))\n",
    "      yfit <- c(NA, head(yfit, n-1))\n",
    "      MSE <- c(MSE, mean((yfit-x)^2, na.rm = T) )\n",
    "    }\n",
    "  }\n",
    "  return(forecast[[which.min(MSE)]])\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "adida = r(\"\"\"\n",
    "function(x, h){\n",
    "  al <- round(mean(intervals(x)),0) #mean inter-demand interval\n",
    "  #Aggregated series (AS)\n",
    "  AS <- as.numeric(na.omit(as.numeric(rollapply(tail(x, (length(x) %/% al)*al), al, FUN=sum, by = al))))\n",
    "  forecast <- rep(SexpS(AS, 1)/al, h)\n",
    "  return(forecast)\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "imapa = r(\"\"\"\n",
    "function(x, h){\n",
    "  mal <- round(mean(intervals(x)),0)\n",
    "  frc <- NULL\n",
    "  for (al in 1:mal){\n",
    "    frc <- rbind(frc, rep(SexpS(as.numeric(na.omit(as.numeric(rollapply(tail(x, (length(x) %/% al)*al), al, FUN=sum, by = al)))), 1)/al, h))\n",
    "  }\n",
    "  forecast <- colMeans(frc)\n",
    "  return(forecast)\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### rpy2 models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "<<rpy2_models>>\n",
    "\n",
    "class SExpS(TimeSeriesModel):\n",
    "    def fit_predict(\n",
    "        self,\n",
    "        y_train: np.ndarray,\n",
    "        y_test: pd.Series,\n",
    "        xreg_train: Union[np.ndarray, NULLType],\n",
    "        xreg_test: Union[np.ndarray, NULLType],\n",
    "    ) -> DataFrame:\n",
    "        try:\n",
    "            y_hat = r_models_functions.sexps(y_train, y_test.shape[0])\n",
    "        except Exception:\n",
    "            raise ModelNotFitError\n",
    "        df_forecast = DataFrame(y_hat, columns=[y_test.name], index=y_test.index)\n",
    "        return df_forecast\n",
    "\n",
    "class Croston(TimeSeriesModel):\n",
    "    def __init__(self, type='classic'):\n",
    "        super().__init__()\n",
    "        self.type = type\n",
    "\n",
    "    def fit_predict(\n",
    "        self,\n",
    "        y_train: np.ndarray,\n",
    "        y_test: pd.Series,\n",
    "        xreg_train: Union[np.ndarray, NULLType],\n",
    "        xreg_test: Union[np.ndarray, NULLType],\n",
    "    ) -> DataFrame:\n",
    "        try:\n",
    "            y_hat = r_models_functions.croston(y_train, y_test.shape[0], self.type)\n",
    "        except Exception:\n",
    "            raise ModelNotFitError\n",
    "        df_forecast = DataFrame(y_hat, columns=[y_test.name], index=y_test.index)\n",
    "        return df_forecast\n",
    "\n",
    "class TSB(TimeSeriesModel):\n",
    "    def fit_predict(\n",
    "        self,\n",
    "        y_train: np.ndarray,\n",
    "        y_test: pd.Series,\n",
    "        xreg_train: Union[np.ndarray, NULLType],\n",
    "        xreg_test: Union[np.ndarray, NULLType],\n",
    "    ) -> DataFrame:\n",
    "        try:\n",
    "            y_hat = r_models_functions.tsb(y_train, y_test.shape[0])\n",
    "        except Exception:\n",
    "            raise ModelNotFitError\n",
    "        df_forecast = DataFrame(y_hat, columns=[y_test.name], index=y_test.index)\n",
    "        return df_forecast\n",
    "\n",
    "class ADIDA(TimeSeriesModel):\n",
    "    def fit_predict(\n",
    "        self,\n",
    "        y_train: np.ndarray,\n",
    "        y_test: pd.Series,\n",
    "        xreg_train: Union[np.ndarray, NULLType],\n",
    "        xreg_test: Union[np.ndarray, NULLType],\n",
    "    ) -> DataFrame:\n",
    "        try:\n",
    "            y_hat = r_models_functions.adida(y_train, y_test.shape[0])\n",
    "        except Exception:\n",
    "            raise ModelNotFitError\n",
    "        df_forecast = DataFrame(y_hat, columns=[y_test.name], index=y_test.index)\n",
    "        return df_forecast\n",
    "\n",
    "class iMAPA(TimeSeriesModel):\n",
    "    def fit_predict(\n",
    "        self,\n",
    "        y_train: np.ndarray,\n",
    "        y_test: pd.Series,\n",
    "        xreg_train: Union[np.ndarray, NULLType],\n",
    "        xreg_test: Union[np.ndarray, NULLType],\n",
    "    ) -> DataFrame:\n",
    "        try:\n",
    "            y_hat = r_models.imapa(y_train, y_test.shape[0])\n",
    "        except Exception:\n",
    "            raise ModelNotFitError\n",
    "        df_forecast = DataFrame(y_hat, columns=[y_test.name], index=y_test.index)\n",
    "        return df_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### CV2, ADI features and classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "• Smooth demand: regular demand over time with a limited vari- ation in quantity;\n",
    "• Intermittent demand: extremely sporadic demand, with no ac- centuated\n",
    "variability in the quantity of the single demand;\n",
    "• Erratic demand: regular distribution over time, but large varia- tion in\n",
    "quantity;\n",
    "• Lumpy demand: extremely sporadic demand, great number of zero-demand periods and large variation in quantity\n",
    "\n",
    "According to Kostenko and Hyndman(2006), an approximate rule for selecting\n",
    "Croston’s methodover SBA is given by v≤2−(3/2)p,where v denotes the square of the\n",
    "coefficient of variation of the demandsand p refers to the meanvalue of the\n",
    "intervals. Graphs (e) through (h) refer to the SBC-KH-SES, where we propose theuse of SES in the casep= 1, irrelevant of the value ofv.\n",
    "\n",
    "-   MAPA produces forecast by averaging forecasts at different frequencies\n",
    "    (aggregation level of 8 periods) (good for longer horizon?)\n",
    "\n",
    "if smooth -> SES\n",
    "if lumpy or intermittent -> SBA\n",
    "if erratic -> croston\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def add_cv2_feature(sales: pd.DataFrame):\n",
    "    d_cols = [c for c in sales.columns if c.startswith('d_')]\n",
    "    _sales = sales.replace({0: pd.NA})\n",
    "    sales['cv2'] = (_sales[d_cols].std(axis=1) / _sales[d_cols].mean(axis=1)) ** 2\n",
    "    return sales\n",
    "\n",
    "def add_adi_feature(sales: pd.DataFrame):\n",
    "    d_cols = [c for c in sales.columns if c.startswith('d_')]\n",
    "    sales['adi'] = sales.shape[1] / np.count_nonzero(sales[d_cols], axis=1)\n",
    "    return sales\n",
    "\n",
    "def add_demand_type_feature(sales: pd.DataFrame):\n",
    "    sales = add_cv2_feature(sales)\n",
    "    sales = add_adi_feature(sales)\n",
    "\n",
    "    sales.loc[(sales['adi']>4/3) & (sales['cv2']>0.5), 'type'] = 'lumpy'\n",
    "    sales.loc[(sales['adi']>4/3) & (sales['cv2']<=0.5), 'type'] = 'intermittent'\n",
    "    sales.loc[(sales['adi']<=4/3) & (sales['cv2']>0.5), 'type'] = 'erratic'\n",
    "    sales.loc[(sales['adi']<=4/3) & (sales['cv2']<=0.5), 'type'] = 'smooth'\n",
    "    return sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Forecast all bottom level series\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from m5.features import add_demand_type_feature\n",
    "from m5.hierarchy import get_sales_long_with_nan, all_names\n",
    "from rpy2_models.ts_model import SExpS, Croston\n",
    "from rpy2_models import r_models_functions\n",
    "\n",
    "\n",
    "def get_y_train_val(series: pd.Series, val_days=28):\n",
    "    y_train = series.dropna()\n",
    "    y_val = series.iloc[-val_days:]\n",
    "    return y_train, y_val\n",
    "\n",
    "\n",
    "def forecast_bottom_level(sales: pd.DataFrame, prices: pd.DataFrame, calendar: pd.DataFrame):\n",
    "    sales_with_type = add_demand_type_feature(sales.copy())\n",
    "    sales_with_type = sales_with_type.set_index(all_names[1:])\n",
    "\n",
    "    sales_long = get_sales_long_with_nan(sales, prices, calendar)\n",
    "\n",
    "    y_pred_long = []\n",
    "    for _, series in sales_long.iteritems():\n",
    "        series_name = series.name[1:]\n",
    "        series_type = sales_with_type.loc[series_name]['type']\n",
    "        if series_type in ['lumpy', 'intermittent']:\n",
    "            model = Croston(type='sba')\n",
    "        elif series_type == 'erratic':\n",
    "            model = Croston(type='optimized')\n",
    "        else:\n",
    "            model = SExpS()\n",
    "\n",
    "        y_train, y_val = get_y_train_val(series)\n",
    "        y_pred = model.fit_predict(y_train.values, y_val, None, None)\n",
    "        y_pred_long.append(y_pred)\n",
    "\n",
    "    y_pred_long = pd.concat(y_pred_long, axis=1)\n",
    "    return y_pred_long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### occurence exp smoothing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "r(\"library(smooth)\")\n",
    "\n",
    "fit_oes = r(\"\"\"\n",
    "    function(y_train, xreg=NULL){\n",
    "        ts(y_train, frequency=7) %>%\n",
    "            oes(model=\"YYY\", occurence=\"auto\", xreg=xreg)\n",
    "    }\n",
    "\"\"\")\n",
    "\n",
    "predict_oes = r(\"\"\"\n",
    "    function(model, h, xreg=NULL){\n",
    "        forecast(model, h=h, xreg=xreg, interval=\"n\") %>%\n",
    "            data.frame()\n",
    "    }\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class OES(TimeSeriesModel):\n",
    "    def fit(self, y_train: np.ndarray, xreg_train: np.ndarray) -> TimeSeriesModel:\n",
    "        self.model = r_models_functions.fit_oes(y_train, xreg_train)\n",
    "        self.is_fitted_ = True\n",
    "        self.fitted = r_models_functions.get_fitted_values(self.model)\n",
    "        return self.model\n",
    "\n",
    "    def predict(self, horizon_max: int, xreg_test: np.ndarray) -> pd.DataFrame:\n",
    "        check_is_fitted(self, \"is_fitted_\")\n",
    "        forecast = r_models_functions.predict_oes(self.model, horizon_max, xreg_test)\n",
    "        forecast = pd.DataFrame(forecast['Point.Forecast'], columns=['prediction'])\n",
    "        return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from rpy2.robjects import NULL\n",
    "\n",
    "model = OES()\n",
    "model.fit(one.values, NULL)\n",
    "model.predict(28, NULL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Ensembling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "&ldquo;Let us consider thesimplest case where the point forecasts of two methods are\n",
    "combined with equal weights.In orderfor this to improve forecasting performance,\n",
    "the point forecasts of the two methods should lie in theopposite sides of the\n",
    "future actual values. Otherwise, if the point forecasts are both lower or\n",
    "greaterthan the actual, then the final forecast error is equal\n",
    "to the simple average of the forecast errorof the two methods.\n",
    "In the majority of cases, these demand rate forecasts are collectively\n",
    "eitherunderestimating the observed demand when a non-zero demand is recorded, or\n",
    "overestimating it,when the observed\n",
    "demand is zero due to the intermittent nature of the data&rdquo;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Top-level univariate models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "groups = {'total': ['d']}\n",
    "\n",
    "sales_long = get_sales_long(sales, groups, include_bottom_level=False)\n",
    "sales_long.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "calendar = add_snap_feature(calendar)\n",
    "calendar = add_special_event_feature(calendar)\n",
    "\n",
    "features = ['snap', 'special_event']\n",
    "input = sales_long.merge(calendar[['d'] + features].set_index('d'), left_index=True, right_index=True)\n",
    "\n",
    "y, X = input.iloc[:, 0], input[features]\n",
    "\n",
    "y_train, X_train = y.iloc[:-28], X.iloc[:-28 :]\n",
    "y_val, X_val = y.iloc[-28:], X.iloc[-28:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### pmdarima\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pmdarima.pipeline import Pipeline\n",
    "from pmdarima.preprocessing import BoxCoxEndogTransformer\n",
    "import pmdarima as pm\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"boxcox\", BoxCoxEndogTransformer()),\n",
    "    (\"model\", pm.AutoARIMA(m=7, seasonal=True, trace=True, error_action='ignore', suppress_warnings=True, stepwise=True))\n",
    "])\n",
    "\n",
    "pipeline.fit(y=y_train, exogenous=X_train)\n",
    "y_pred = pipeline.predict(n_periods=X_val.shape[0], exogenous=X_val)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "pd.DataFrame(data=y_pred, columns= ['pred'], index=X_val.index).plot(ax=ax)\n",
    "y_val.to_frame().plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### rpy2 auto.arima\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from rpy2_models.ts_model_cv import ARIMAXCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = ARIMAXCV(seasonal=True)\n",
    "model.find_model_structure(y_train = y_train.values, xreg_train=X_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(horizon_max = X_val.shape[0], xreg_test = X_val.values)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "y_pred[['prediction']].plot(ax=ax)\n",
    "y_val.to_frame().plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### exponential smoothing with exogenous variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "r(\"library(smooth)\")\n",
    "\n",
    "fit_esx = r(\"\"\"\n",
    "    function(y_train, xreg){\n",
    "        ts(y_train, frequency=7) %>%\n",
    "            es(xreg, occurence=\"auto\")\n",
    "    }\n",
    "\"\"\")\n",
    "\n",
    "predict_esx = r(\"\"\"\n",
    "    function(model, h, xreg){\n",
    "        forecast(model, h=h, xreg=xreg, interval='n') %>%\n",
    "            data.frame()\n",
    "    }\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ESX(TimeSeriesModel):\n",
    "    def fit(self, y_train: np.ndarray, xreg_train: np.ndarray) -> TimeSeriesModel:\n",
    "        self.model = r_models_functions.fit_esx(y_train, xreg_train)\n",
    "        self.is_fitted_ = True\n",
    "        self.fitted = r_models_functions.get_fitted_values(self.model)\n",
    "        return self.model\n",
    "\n",
    "    def predict(self, horizon_max: int, xreg_test: np.ndarray) -> pd.DataFrame:\n",
    "        check_is_fitted(self, \"is_fitted_\")\n",
    "        forecast = r_models_functions.predict_esx(self.model, horizon_max, xreg_test)\n",
    "        forecast = pd.DataFrame(forecast['Point.Forecast'], columns=['prediction'])\n",
    "        return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = ESX()\n",
    "model.fit(y_train, X_train.values)\n",
    "y_pred = model.predict(horizon_max = X_val.shape[0], xreg_test = X_val.values)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "y_pred[['prediction']].plot(ax=ax)\n",
    "y_val.to_frame().plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\u001b[0m\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### prophet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from fbprophet import Prophet\n",
    "\n",
    "df = sales_long.iloc[:-28, 0:1].merge(calendar[['d', 'date']].set_index('d'), left_index=True, right_index=True)\n",
    "df.columns = ['y', 'ds']\n",
    "m = Prophet()\n",
    "m.fit(df)\n",
    "\n",
    "future = m.make_future_dataframe(periods=28, include_history=False)\n",
    "forecast = m.predict(future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "forecast[['ds', 'yhat']].set_index('ds').plot(ax=ax)\n",
    "pd.DataFrame(sales_long.iloc[-28:, 0].values, index=forecast['ds']).plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### try auto-ces/ssarima/smoothCombine\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### try ves (and covar)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def get_sales_and_price_wide(sales, prices, calendar):\n",
    "    # map days to dates\n",
    "    day_to_date = calendar.set_index(\"d\")[\"date\"].to_dict()\n",
    "    sales_wide = sales.rename(columns=day_to_date)\n",
    "    # select dates\n",
    "    all_dates = sales_wide.select_dtypes('int').columns\n",
    "    sales_wide = sales_wide.set_index(['item_id', 'store_id'])[all_dates]\n",
    "    sales_wide.columns = pd.to_datetime(sales_wide.columns).rename('date')\n",
    "    # add date to prices\n",
    "    prices_wide = prices.merge(calendar[['date', 'wm_yr_wk']]).drop(columns=['wm_yr_wk']).set_index(['date', 'item_id', 'store_id']).unstack(level='date').sort_index()\n",
    "    prices_wide.columns = prices_wide.columns.droplevel(0)\n",
    "    prices_wide.columns = pd.to_datetime(prices_wide.columns).rename('date')\n",
    "    # make sure prices_wide and sales_wide have same index\n",
    "    # TODO what about prices for future dates?\n",
    "    prices_wide = prices_wide.loc[prices_wide.index.intersection(sales_wide.index), sales_wide.columns.intersection(sales_wide.columns)]\n",
    "    # set demand to NA in dates where a product was not available\n",
    "    sales_wide[prices_wide.isna()] = pd.NA\n",
    "\n",
    "    return prices_wide, sales_wide\n",
    "\n",
    "def compute_float_features(sales_wide, prices_wide, forecast_horizon = 28):\n",
    "    all_features_wide = {}\n",
    "    for lag in tqdm([0, 7, 28], \"autoregressive features\"):\n",
    "        autoregressive = sales_wide.shift(forecast_horizon + lag, axis=1)\n",
    "        all_features_wide[f\"ar_{lag}\"] = autoregressive\n",
    "\n",
    "    for window in tqdm([7, 30, 60, 90, 180], \"rolling features\"):\n",
    "        mean = sales_wide.shift(forecast_horizon, axis=1).rolling(window, axis=1).mean()\n",
    "        all_features_wide[f\"mean_{window}\"] = mean\n",
    "\n",
    "        std = sales_wide.shift(forecast_horizon, axis=1).rolling(window, axis=1).std()\n",
    "        all_features_wide[f\"std_{window}\"] = std\n",
    "\n",
    "        ewma = sales_wide.shift(forecast_horizon, axis=1).ewm(span=window, min_periods=window).mean()\n",
    "        all_features_wide[f\"ewma_{window}\"] = ewma\n",
    "\n",
    "    for window in tqdm([30], \"skew, kurt features\"):\n",
    "        skew = sales_wide.shift(forecast_horizon, axis=1).rolling(window, axis=1).skew()\n",
    "        all_features_wide[f\"skew_{window}\"] = skew\n",
    "\n",
    "        kurt = sales_wide.shift(forecast_horizon, axis=1).rolling(window, axis=1).kurt()\n",
    "        all_features_wide[f\"kurt_{window}\"] = kurt\n",
    "\n",
    "    for window in tqdm([7, 30], \"price std features\"):\n",
    "        price_std = prices_wide.rolling(window, axis=1).std()\n",
    "        all_features_wide[f\"price_std_{window}\"] = price_std\n",
    "\n",
    "    for window in tqdm([365], \"price change features\"):\n",
    "        price_max_year = prices_wide.rolling(window, axis=1).max()\n",
    "        price_change_year = (prices_wide - price_max_year) / price_max_year\n",
    "        price_change_week = (prices_wide - prices_wide.shift(1, axis=1)) / prices_wide.shift(1, axis=1)\n",
    "\n",
    "        all_features_wide[f\"price_max_365\"] = price_max_year\n",
    "        all_features_wide[f\"price_change_365\"] = price_change_year\n",
    "        all_features_wide[f\"price_change_7\"] = price_change_week\n",
    "\n",
    "    return float_features_wide\n",
    "\n",
    "def get_train_val_dates(float_features_wide, val_days, only_val):\n",
    "    train_val_dates = list(float_features_wide.values())[0].columns\n",
    "    # only include dates for which all features have at least one observation\n",
    "    for _, f in tqdm(float_features_wide.items(), \"computing train and validation index\"):\n",
    "        train_val_dates = train_val_dates.intersection(f.dropna(axis=1, how='all').columns)\n",
    "\n",
    "    train_dates = train_val_dates[:-val_days] if not only_val else pd.DatetimeIndex([])\n",
    "    val_dates = train_val_dates[-val_days:]\n",
    "\n",
    "    return train_dates, val_dates\n",
    "\n",
    "def stack_float_features(float_features_wide, train_dates, val_dates):\n",
    "    train_val_dates = train_dates.union(val_dates)\n",
    "\n",
    "    float_features_names = [*float_features_wide.keys()]\n",
    "    all_features_long = []\n",
    "    for name in tqdm(float_features_names, \"building design matrix\"):\n",
    "        f = float_features_wide.pop(name)  # casting before stacking is efficient\n",
    "        f = f.loc[:, train_val_dates].astype(np.float32).stack(dropna=False).sort_index() # sort index so we are sure all features are aligned\n",
    "        all_features_long.append(f.values[:, np.newaxis]) # convert 1D array to column vector\n",
    "    all_features_long = pd.DataFrame(data=np.hstack(all_features_long), index=f.index, columns=float_features_names)\n",
    "    all_features_long.index = all_features_long.index.rename('date', level=-1)\n",
    "\n",
    "    return all_features_long\n",
    "\n",
    "def add_datetime_features(train_dates, val_dates, calendar):\n",
    "    train_val_dates = train_dates.union(val_dates)\n",
    "\n",
    "    datetime_features = calendar[['date', 'event_name_1', 'event_type_1', 'snap_CA', 'snap_TX', 'snap_WI']].set_index('date').loc[train_val_dates]\n",
    "\n",
    "    datetime_attrs = [\n",
    "        \"quarter\",\n",
    "        \"month\",\n",
    "        \"week\",\n",
    "        \"day\",\n",
    "        \"dayofweek\",\n",
    "        \"is_year_end\",\n",
    "        \"is_year_start\",\n",
    "        \"is_quarter_end\",\n",
    "        \"is_quarter_start\",\n",
    "        \"is_month_end\",\n",
    "        \"is_month_start\",\n",
    "    ]\n",
    "    for attr in tqdm(datetime_attrs, \"datetime features\"):\n",
    "        datetime_features.loc[:, attr] = getattr(datetime_features.index, attr)\n",
    "        # cast int features to int8, the others are categorical\n",
    "        if not attr.startswith('is'):\n",
    "            datetime_features[attr] = datetime_features[attr].astype('int8')\n",
    "    datetime_features.loc[:, \"is_weekend\"] = datetime_features.index.dayofweek.isin([5, 6])\n",
    "    datetime_features.loc[:, \"year\"] = datetime_features.index.year\n",
    "\n",
    "    return datetime_features\n",
    "\n",
    "def get_X_and_y(sales: pd.DataFrame, prices: pd.DataFrame, calendar: pd.DataFrame, cat_features: List[str], val_days=28, only_val=False):\n",
    "    sales_wide, prices_wide = get_sales_and_price_wide(sales, prices, calendar)\n",
    "\n",
    "    float_features_wide = compute_float_features(sales_wide, prices_wide)\n",
    "\n",
    "    train_dates, val_dates = get_train_val_dates(float_features_wide, val_days, only_val)\n",
    "    # test_dates = prices_wide.iloc[:, -28:]\n",
    "\n",
    "    all_features_long = stack_float_features(float_features_wide, train_dates, val_dates)\n",
    "\n",
    "    # set index as columns to add item_id, store_id and date\n",
    "    all_features_long = all_features_long.assign(**all_features_long.index.to_frame())\n",
    "\n",
    "    datetime_features = add_datetime_features(train_dates, val_dates, calendar)\n",
    "    all_features_long = all_features_long.merge(datetime_features, left_index=True, right_index=True)\n",
    "    all_features_long[\"date\"] = all_features_long[\"date\"].astype('int')\n",
    "\n",
    "    # cast categories\n",
    "    all_features_long[cat_features] = all_features_long[cat_features].astype('category')\n",
    "\n",
    "    # TODO add function that computes X_val and X_test by shifting with the forecast horizon\n",
    "    # TODO dropna on num features\n",
    "    sales_long = sales_wide.loc[:, train_dates.union(val_dates)].stack(dropna=False).sort_index()\n",
    "    sales_long.index = sales_long.index.rename('date', level=-1)\n",
    "\n",
    "    y_train, X_train = sales_long.loc(axis=0)[:, :, train_dates], all_features_long.loc(axis=0)[:, :, train_dates]\n",
    "    y_val, X_val = sales_long.loc(axis=0)[:, :, val_dates], all_features_long.loc(axis=0)[:, :, val_dates]\n",
    "    # X_test = all_features_long.loc(axis=0)[:, :, test_dates]\n",
    "\n",
    "    return (X_train, y_train), (X_val, y_val)#, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### more features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratio_features(df):\n",
    "\n",
    "    df['LastWeekByPrevWeek'] = get_ratio(df['LastWeekValues'],df['PrevWeekValues'])\n",
    "    df['Last2WeeksAverage'] = ((df['LastWeekValues']+df['PrevWeekValues'])/2.0).fillna(df['LastWeekValues'])\n",
    "    df['LastWeekByForecastMean'] = get_ratio(df['LastWeekValues'],df['MeanByForecastId'])\n",
    "    df['LastWeekBySiteMean'] = get_ratio(df['LastWeekValues'],df['MeanBySiteId'])\n",
    "    df['ForecastMeanBySiteMean'] = get_ratio(df['MeanByForecastId'],df['MeanBySiteId'])\n",
    "    df['ForecastMedianBySiteMedian'] = get_ratio(df['MedianByForecastId'],df['MedianBySiteId'])\n",
    "    df['LastWeekBySiteMax'] = get_ratio(df['LastWeekValues'], df['MaxBySiteId'])\n",
    "    df['LastWeekBySiteMin'] = get_ratio(df['LastWeekValues'], df['MinBySiteId'])\n",
    "    df['SiteMaxBySiteMin'] = get_ratio(df['MaxBySiteId'], df['MinBySiteId'])\n",
    "    df['SiteMeanBySiteMedian'] = get_ratio(df['MeanBySiteId'], df['MedianBySiteId'])\n",
    "    df['LastDayByPrev'] = get_ratio(df['LastDayValues'], df['MeanBySiteIdTime1D_offset1W'])\n",
    "    df['LastHourByPrev'] = get_ratio(df['MeanBySiteId1h'], df['MeanBySiteId1h_offset1W'])\n",
    "    df['CurTempMinusMonthMedian'] = (df['Temperature'] - df['MedianTemperatureBySiteIdTime1M']).fillna(0)\n",
    "\n",
    "#    df['LastDayBySiteMean'] = get_ratio(df['LastDayValues'],df['MeanBySiteId10Y'])\n",
    "#    df['LastHourBySiteMean'] = get_ratio(df['MeanBySiteId1h'],df['MeanBySiteId10Y'])\n",
    "\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tpot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from tpot import TPOTRegressor\n",
    "from joblib import dump\n",
    "\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "from m5.global_model import compute_features, get_X_y\n",
    "\n",
    "data = Path('data/')\n",
    "sales = pd.read_csv(data / 'sales_train_validation.csv', dtype={\n",
    "    'item_id':'category', 'dept_id':'category', 'cat_id':'category', 'store_id':'category', 'state_id':'category'\n",
    "})\n",
    "prices = pd.read_csv(data / 'sell_prices.csv', dtype={\n",
    "    'store_id': 'category', 'item_id': 'category'\n",
    "})\n",
    "calendar = pd.read_csv(data / 'calendar.csv', parse_dates=['date'], dtype={\n",
    "    'weekday':'category', 'd':'category', 'event_name':'category', 'event_type_1':'category', 'event_type_2':'category',\n",
    "    'snap_CA':'category', 'snap_TX':'category', 'snap_WI':'category'\n",
    "})\n",
    "\n",
    "sales_long = compute_features(sales, prices, calendar)\n",
    "\n",
    "cat_features = ['item_id', 'store_id',\n",
    "            'event_name_1', 'event_type_1', 'snap_CA', 'snap_TX', 'snap_WI',\n",
    "            'is_year_end', 'is_year_start', 'is_month_start', 'is_month_end', 'is_quarter_start', 'is_quarter_end', 'is_weekend']\n",
    "\n",
    "X_train, y_train, X_val, y_val = get_X_y(sales_long, cat_features)\n",
    "\n",
    "model = TPOTRegressor(generations=5, population_size=50, verbosity=2, random_state=42, config_dict='TPOT light')\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "dump(model, 'tpot.joblib')\n",
    "print(model.export())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('src')\n",
    "from m5.global_model import get_X_and_y\n",
    "from m5.hierarchy import compute_series_weights\n",
    "\n",
    "\n",
    "data = Path('data/')\n",
    "sales = pd.read_csv(data / 'sales_train_validation.csv', dtype={\n",
    "    'item_id':'category', 'dept_id':'category', 'cat_id':'category', 'store_id':'category', 'state_id':'category'\n",
    "})\n",
    "prices = pd.read_csv(data / 'sell_prices.csv', dtype={\n",
    "    'store_id': 'category', 'item_id': 'category'\n",
    "})\n",
    "calendar = pd.read_csv(data / 'calendar.csv', parse_dates=['date'], dtype={\n",
    "    'weekday':'category', 'd':'category', 'event_name':'category', 'event_type_1':'category', 'event_type_2':'category',\n",
    "    'snap_CA':'category', 'snap_TX':'category', 'snap_WI':'category'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['item_id', 'store_id',\n",
    "            'event_name_1', 'event_type_1', 'snap_CA', 'snap_TX', 'snap_WI',\n",
    "            'is_year_end', 'is_year_start', 'is_month_start', 'is_month_end', 'is_quarter_start', 'is_quarter_end', 'is_weekend']\n",
    "\n",
    "X_train, y_train, X_val, y_val = get_X_and_y(sales.iloc[:100, :], prices, calendar, cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_plot = y_train.unstack('date').iloc[4, :52]\n",
    "y_plot.index = y_plot.index.droplevel(0)\n",
    "y_plot.to_frame().plot(figsize=(20,10))\n",
    "X_train.unstack('date').loc[:, 'ewma_30'].iloc[4, :52].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = lgb.Dataset(X_train, y_train, categorical_feature = cat_features, free_raw_data=False)\n",
    "val_set = lgb.Dataset(X_val, y_val, categorical_feature = cat_features, reference=train_set, free_raw_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'metric': 'l2', 'objective': 'regression','seed': 23}\n",
    "\n",
    "model = lgb.train(params, train_set, num_boost_round = 5000, early_stopping_rounds=100,\n",
    "                  valid_sets = [train_set, val_set], verbose_eval = 100)\n",
    "model.save_model('model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "y_hat_long = pd.DataFrame(preds, index=y_val.index, columns=['prediction'])\n",
    "y_hat_long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled squared errors as custom loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_weights(y: pd.Series, normalize=False):\n",
    "    \"\"\"\n",
    "    For each series, compute the denominator in the MSSE loss function, i.e. the\n",
    "    day-to-day variations squared, averaged by number of training observations.\n",
    "    The weights can be normalized so that they add up to 1.\n",
    "    This is provided to the lgb.Dataset for computing loss function and evaluation metric\n",
    "    \"\"\"\n",
    "    scales = (y.unstack(level='date').diff(axis=1) ** 2).mean(axis=1)\n",
    "    scales = scales.replace(0, pd.NA)\n",
    "    weights = 1 / scales\n",
    "    if normalize:\n",
    "        weights = weights.divide(weights.sum())\n",
    "    weights = y.merge(weights.to_frame('weight'), left_index=True, right_index=True)['weight']\n",
    "    return weights\n",
    "\n",
    "train_weights = get_y_weights(y_train)\n",
    "val_weights = get_y_weights(y_val)\n",
    "\n",
    "train_set = lgb.Dataset(X_train, y_train, weight=train_weights, categorical_feature = cat_features, free_raw_data=False)\n",
    "val_set = lgb.Dataset(X_val, y_val, weight=val_weights, categorical_feature = cat_features, reference=train_set, free_raw_data=False)\n",
    "\n",
    "def sse(preds, train_data):\n",
    "    true = train_data.get_label()\n",
    "    weights = train_data.get_weight() # weights is 1 / scale, normalized\n",
    "    # loss = weights * (preds - true) ** 2\n",
    "    gradient = weights * 2 * (preds - true)\n",
    "    hessian = weights * 2\n",
    "    return gradient, hessian\n",
    "\n",
    "# model = lgb.train(params, train_set, num_boost_round = 5000, early_stopping_rounds=100,\n",
    "#                   valid_sets = [train_set, val_set], verbose_eval = 100, fobj=sse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WRMSSE as custom metric\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = compute_series_weights(sales, prices, calendar, groups={})\n",
    "\n",
    "def get_wrmsse_lgb(preds, data):\n",
    "    if preds.shape[0] == y_val.shape[0]:\n",
    "        y = y_val.copy()\n",
    "    else:\n",
    "        y = y_train.copy()\n",
    "\n",
    "    y['prediction'] = preds\n",
    "    y['weight'] = data.get_weight() # weights is 1 / scale, normalized\n",
    "    y['sse'] = ((y['sales'] - y['prediction']) ** 2) * y['weight']\n",
    "\n",
    "    rmsse = y[['sse']].unstack(level='date').mean(axis=1).pow(1/2).to_frame(name='rmsse')\n",
    "    rmsse =  rmsse.merge(weights['series_weight'], left_index=True, right_index=True)\n",
    "    wrmsse = rmsse['rmsse'] * rmsse['series_weight']\n",
    "\n",
    "    score = wrmsse.sum()\n",
    "    return 'wrmsse', score, False\n",
    "\n",
    "model = lgb.train(params, train_set, num_boost_round = 5000, early_stopping_rounds=100,\n",
    "                  valid_sets = [train_set, val_set], verbose_eval = 100, fobj=sse, feval=get_wrmsse_lgb)\n",
    "model.save_model('model_custom_loss.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance (and deselecting the unimportant ones)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = lgb.Booster(model_file='data/models/model.txt')\n",
    "ax = plot_importance(model, max_num_features=20, figsize=(20,10))\n",
    "plt.show()\n",
    "plt.savefig(data / 'lgbm_importance.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join((\"%s: %.2f\" % x) for x in sorted(\n",
    "    zip(train_sub[use_cols].columns, bst.feature_importance(\"gain\")),\n",
    "    key=lambda x: x[1], reverse=True  )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# sorted(zip(clf.feature_importances_, X.columns), reverse=True)\n",
    "feature_imp = pd.DataFrame(sorted(zip(gbm.feature_importances_,X_val.columns)), columns=['Value','Feature'])\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\n",
    "plt.title('LightGBM feature importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig(data / 'lgbm_importance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train models for different horizons\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'n_estimators': [20, 40]\n",
    "}\n",
    "\n",
    "gbm = GridSearchCV(estimator, param_grid, cv=3)\n",
    "gbm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_grid = {'boosting_type' : ['gbdt'],\n",
    "  'learning_rate' : [0.02, 0.03, 0.04, 0.05, 0.07, 0.1, 0.15, 0.2],\n",
    "  'bagging_freq' : [1, 2, 3, 5, 10, 20, 50, 100],\n",
    "  'feature_fraction' : [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.5, 0.6, 0.7,0.8,0.9,1.0],\n",
    "  'bagging_fraction' : [0.2, 0.5, 0.7,0.8,0.85, 0.9, 0.95, 1.0],\n",
    "  'max_depth' : [4,5,6,7,8,10,12,14,20],\n",
    "  'num_leaves' : [7,15, 31, 63, 127, 255, 511, 1023, 2047],\n",
    "  'min_data_in_leaf' : [1, 10, 20, 30, 50, 70, 100, 120, 150, 200, 300],\n",
    "  'min_sum_hessian_in_leaf' : [0, 0.001, 0.01, 0.1, 1, 3, 10, 30, 100],\n",
    "  'min_gain_to_split' : [0, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "  'lambda_l1' : [0, 0.001, 0.01, 0.03, 0.1, 0.3, 1, 10, 100],\n",
    "  'lambda_l2' : [0, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "  'max_bin' : [63, 127, 255, 511, 1023, 2047],\n",
    "  'use_onehot' : [True, False],\n",
    "  'my_keep_mean_nan' : [True],\n",
    "  'my_skip_first' : [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8],\n",
    "  'my_log' : [True, False],\n",
    "  'my_weights' : ['no', 'raw', 'norm'],\n",
    "  'objective' : ['regression_l2','regression_l1','huber','fair'],\n",
    "  'metric' : ['lgb_nwrmse']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = lgb.cv(\n",
    "        params,\n",
    "        dftrainLGB,\n",
    "        num_boost_round=100,\n",
    "        nfold=3,\n",
    "        metrics='mae',\n",
    "        early_stopping_rounds=10,\n",
    "\n",
    "        # This is what I added\n",
    "        stratified=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate forecast on all levels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = pd.read_hdf(data / 'y_hat_bottom_level.h5', 'y_hat_bottom_level')\n",
    "sales_long = pd.read_hdf(data / 'sales_long.h5', key='sales_long')\n",
    "weights = pd.read_hdf(data / 'weights_all_levels.h5', 'weights_all_levels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_level_wrmsse = get_wrmsse(y_hat, sales_long, weights)\n",
    "bottom_level_wrmsse * 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.loc[rmsse.index]['series_weight'] * rmsse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSSE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\boldsymbol{R M S S E}=\\sqrt{\\frac{1}{h} \\frac{\\sum_{t=n+1}^{n+h}\\left(Y_{t}-\\bar{Y}_{t}\\right)^{2}}{\\frac{1}{n-1} \\sum_{t=2}^{n}\\left(Y_{t}-Y_{t-1}\\right)^{2}}} $\n",
    "\n",
    "-   Numerator: squared errors, averaged across all horizons\n",
    "-   Denominator: day-to-day variations squared, averaged by number of training observations\n",
    "-   The squared metric is then passed through sqrt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmsse(y_hat: pd.DataFrame, sales_long: pd.DataFrame):\n",
    "    y_true = sales_long.loc[y_hat.index, y_hat.columns]\n",
    "    y_train = sales_long.loc[sales_long.index.difference(y_hat.index), y_hat.columns]\n",
    "\n",
    "    mse = ((y_hat - y_true) ** 2).mean(axis=0)\n",
    "    scale = ((y_train.diff(axis=0)) ** 2).mean(axis=0)\n",
    "    rmsse = np.sqrt(mse / scale)\n",
    "    return rmsse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WRMSSE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](/Users/luca/git/org/.attach/6D/F60BC8-A965-4C97-ADCA-549C5CAD7A48/_20200320_144903Screenshot 2020-03-20 at 14.48.58.png)\n",
    "\n",
    "Once we have sales<sub>long</sub> and prices<sub>long</sub> in the same format, it should be\n",
    "trivial to calculate for each series:\n",
    "\n",
    "-   its dollar value in the last 28 days\n",
    "-   the dollar value of its hierarchy in the last 28 days\n",
    "\n",
    "Once computed, the weights can be persisted and reused\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wrmsse(y_hat: pd.DataFrame, sales_long: pd.DataFrame, weights: pd.DataFrame):\n",
    "    rmsse = get_rmsse(y_hat, sales_long)\n",
    "    wrmsse = (weights.loc[rmsse.index]['series_weight'] * rmsse).sum()\n",
    "    return wrmsse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle reference\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm as tqdm\n",
    "\n",
    "class WRMSSEEvaluator(object):\n",
    "\n",
    "    group_ids = ( 'all_id', 'state_id', 'store_id', 'cat_id', 'dept_id', 'item_id',\n",
    "        ['state_id', 'cat_id'],  ['state_id', 'dept_id'], ['store_id', 'cat_id'],\n",
    "        ['store_id', 'dept_id'], ['item_id', 'state_id'], ['item_id', 'store_id'])\n",
    "\n",
    "    def __init__(self,\n",
    "                 train_df: pd.DataFrame,\n",
    "                 valid_df: pd.DataFrame,\n",
    "                 calendar: pd.DataFrame,\n",
    "                 prices: pd.DataFrame):\n",
    "        '''\n",
    "        intialize and calculate weights\n",
    "        '''\n",
    "        self.calendar = calendar\n",
    "        self.prices = prices\n",
    "        self.train_df = train_df\n",
    "        self.valid_df = valid_df\n",
    "        self.train_target_columns = [i for i in self.train_df.columns if i.startswith('d_')]\n",
    "        self.weight_columns = self.train_df.iloc[:, -28:].columns.tolist()\n",
    "\n",
    "        self.train_df['all_id'] = \"all\"\n",
    "\n",
    "        self.id_columns = [i for i in self.train_df.columns if not i.startswith('d_')]\n",
    "        self.valid_target_columns = [i for i in self.valid_df.columns if i.startswith('d_')]\n",
    "\n",
    "        if not all([c in self.valid_df.columns for c in self.id_columns]):\n",
    "            self.valid_df = pd.concat([self.train_df[self.id_columns], self.valid_df],\n",
    "                                      axis=1,\n",
    "                                      sort=False)\n",
    "        self.train_series = self.trans_30490_to_42840(self.train_df,\n",
    "                                                      self.train_target_columns,\n",
    "                                                      self.group_ids)\n",
    "        self.valid_series = self.trans_30490_to_42840(self.valid_df,\n",
    "                                                      self.valid_target_columns,\n",
    "                                                      self.group_ids)\n",
    "        self.weights = self.get_weight_df()\n",
    "\n",
    "    def get_name(self, i):\n",
    "        '''\n",
    "        convert a str or list of strings to unique string\n",
    "        used for naming each of 42840 series\n",
    "        '''\n",
    "        if type(i) == str or type(i) == int:\n",
    "            return str(i)\n",
    "        else:\n",
    "            return \"--\".join(i)\n",
    "\n",
    "    def get_weight_df(self) -> pd.DataFrame:\n",
    "        '''\n",
    "        returns weights for each of 42840 series in a dataFrame\n",
    "        '''\n",
    "        day_to_week = self.calendar.set_index('d')['wm_yr_wk'].to_dict()\n",
    "        weight_df = self.train_df[['item_id', 'store_id'] + self.weight_columns].set_index(['item_id', 'store_id'])\n",
    "        weight_df = weight_df.stack().reset_index().rename(columns={'level_2': 'd', 0: 'value'})\n",
    "        weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)\n",
    "        weight_df = weight_df.merge(self.prices, how='left', on=['item_id', 'store_id', 'wm_yr_wk'])\n",
    "        weight_df['value'] = weight_df['value'] * weight_df['sell_price']\n",
    "        weight_df = weight_df.set_index(['item_id', 'store_id', 'd']).unstack(level=2)['value']\n",
    "        weight_df = weight_df.loc[zip(self.train_df.item_id, self.train_df.store_id), :].reset_index(drop=True)\n",
    "        weight_df = pd.concat([self.train_df[self.id_columns], weight_df], axis=1, sort=False)\n",
    "        weights_map = {}\n",
    "        for i, group_id in enumerate(tqdm(self.group_ids, leave=False)):\n",
    "            lv_weight = weight_df.groupby(group_id)[self.weight_columns].sum().sum(axis=1)\n",
    "            lv_weight = lv_weight / lv_weight.sum()\n",
    "            for i in range(len(lv_weight)):\n",
    "                    weights_map[self.get_name(lv_weight.index[i])] = np.array([lv_weight.iloc[i]])\n",
    "        weights = pd.DataFrame(weights_map).T / len(self.group_ids)\n",
    "\n",
    "        return weights\n",
    "\n",
    "    def trans_30490_to_42840(self, df, cols, group_ids):\n",
    "        '''\n",
    "        transform 30490 sries to all 42840 series\n",
    "        '''\n",
    "        series_map = {}\n",
    "        for i, group_id in enumerate(tqdm(self.group_ids, leave=False)):\n",
    "            tr = df.groupby(group_id)[cols].sum()\n",
    "            for i in range(len(tr)):\n",
    "                series_map[self.get_name(tr.index[i])] = tr.iloc[i].values\n",
    "        return pd.DataFrame(series_map).T\n",
    "\n",
    "    def get_rmsse(self, valid_preds) -> pd.Series:\n",
    "        '''\n",
    "        returns rmsse scores for all 42840 series\n",
    "        '''\n",
    "        score = ((self.valid_series - valid_preds) ** 2).mean(axis=1)\n",
    "        scale = ((self.train_series.iloc[:, 1:].values - self.train_series.iloc[:, :-1].values) ** 2).mean(axis=1)\n",
    "        rmsse = (score / scale).map(np.sqrt)\n",
    "        return rmsse\n",
    "\n",
    "    def score(self, valid_preds: Union[pd.DataFrame, np.ndarray]) -> float:\n",
    "        assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape\n",
    "\n",
    "        if isinstance(valid_preds, np.ndarray):\n",
    "            valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns)\n",
    "\n",
    "        valid_preds = pd.concat([self.valid_df[self.id_columns], valid_preds], axis=1, sort=False)\n",
    "        valid_preds = self.trans_30490_to_42840(valid_preds, self.valid_target_columns, self.group_ids)\n",
    "        self.rmsse = self.get_rmsse(valid_preds)\n",
    "        self.contributors = pd.concat([self.weights, self.rmsse], axis=1, sort=False).prod(axis=1)\n",
    "        return np.sum(self.contributors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconciliation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://otexts.com/fpp2/reconciliation.html](https://otexts.com/fpp2/reconciliation.html)\n",
    "$$ \\tilde{y}_h = S G \\hat{y}_h $$, where $G$ is the &rsquo;reconciliation&rsquo; matrix\n",
    "which minimizes the forecast errors of the base forecasts\n",
    "\n",
    "Reconciliation approaches:\n",
    "\n",
    "-   Bottom up (BU)\n",
    "-   Top down:\n",
    "    -   Average historical proportions (AHP)\n",
    "    -   Proportions of historical averages (PHA)\n",
    "    -   Forecast proportions (FP)\n",
    "-   Middle-out\n",
    "-   Optimal (minT):\n",
    "    -   WLS, variance scaling\n",
    "    -   WLS, structural scaling\n",
    "    -   OLS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_matrix = compute_summing_matrix(sales)\n",
    "\n",
    "n_bottom_nodes = sum_matrix.shape[1]\n",
    "n_nodes = sum_matrix.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structurally Weighted Least Squares\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimalComb(forecastsDict, sumMat, method, mse_dict):\n",
    "    if mse_dict is None:\n",
    "        raise Exception('You cannot reconcile optimally without training errors')\n",
    "\n",
    "    global optiMat\n",
    "    hatMat = np.zeros([len(forecastsDict[0].yhat), 1])\n",
    "    for key in forecastsDict.keys():\n",
    "        f1 = np.array(forecastsDict[key].yhat)\n",
    "        f2 = f1[:, np.newaxis]\n",
    "        if np.all(hatMat == 0):\n",
    "            hatMat = f2\n",
    "        else:\n",
    "            hatMat = np.concatenate((hatMat, f2), axis=1)\n",
    "    ##\n",
    "    # Multiply the Summing Matrix Together S*inv(S'S)*S'\n",
    "    ##\n",
    "    if method == \"OLS\":\n",
    "        optiMat = np.dot(np.dot(sumMat, np.linalg.inv(np.dot(np.transpose(sumMat), sumMat))), np.transpose(sumMat))\n",
    "    if method == \"WLSS\":\n",
    "        \"\"\"\n",
    "        optimal combination by Structurally Weighted Least Squares\n",
    "        This specification assumes that the bottom-level base forecast errors each have variance kh and are uncorrelated between nodes\n",
    "        This estimator only depends on the structure of the aggregations, and not on the actual data\n",
    "        \"\"\"\n",
    "        diagMat = np.diag(np.transpose(np.sum(sumMat, axis=1)))\n",
    "        optiMat = np.dot(\n",
    "            np.dot(np.dot(sumMat, np.linalg.inv(np.dot(np.dot(np.transpose(sumMat), np.linalg.inv(diagMat)), sumMat))),\n",
    "                   np.transpose(sumMat)), np.linalg.inv(diagMat))\n",
    "    if method == \"WLSV\":\n",
    "        \"\"\"\n",
    "        optimal combination by Error Variance Weighted Least Squares\n",
    "        This specification scales the base forecasts using the variance of the residuals\n",
    "        \"\"\"\n",
    "        diagMat = [mse_dict[key] for key in mse_dict.keys()]\n",
    "        diagMat = np.diag(np.flip(np.hstack(diagMat) + 0.0000001, 0))\n",
    "        optiMat = np.dot(\n",
    "            np.dot(np.dot(sumMat, np.linalg.inv(np.dot(np.dot(np.transpose(sumMat), np.linalg.inv(diagMat)), sumMat))),\n",
    "                   np.transpose(sumMat)), np.linalg.inv(diagMat))\n",
    "\n",
    "    newMat = np.empty([hatMat.shape[0], sumMat.shape[0]])\n",
    "    for i in range(hatMat.shape[0]):\n",
    "        newMat[i, :] = np.dot(optiMat, np.transpose(hatMat[i, :]))\n",
    "\n",
    "    return newMat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconciliation with htsprophet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/luca/git/htsprophet/htsprophet')\n",
    "from htsprophet.__main__ import forecast_hts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchy nodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_hierarchy(sales: pd.DataFrame):\n",
    "    hierarchy = OrderedDict()\n",
    "    hierarchy['total->state_id'] = {'total': sales['state_id'].unique()}\n",
    "\n",
    "    def fill_level(level, level_child):\n",
    "        hierarchy[level + '->' + level_child] = {}\n",
    "        for unique in sales[level].unique():\n",
    "            hierarchy[level + '->' + level_child][unique] = sales[sales[level] == unique][level_child].unique()\n",
    "        return hierarchy\n",
    "\n",
    "    fill_level('state_id', 'store_id')\n",
    "    fill_level('store_id', 'cat_id')\n",
    "    fill_level('store_id', 'dept_id')\n",
    "\n",
    "    return hierarchy\n",
    "\n",
    "\n",
    "def get_hierarchy_nodes(hierarchy):\n",
    "    nodes = []\n",
    "    for level, level_dict in hierarchy.items():\n",
    "        level_nodes = []\n",
    "        for level_child, level_child_values in level_dict.items():\n",
    "            level_nodes.append(level_child_values.size)\n",
    "        nodes.append(level_nodes)\n",
    "\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### forecast and reconcile with htsprophet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchy = get_hierarchy(sales)\n",
    "nodes = get_hierarchy_nodes(hierarchy)\n",
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays = (calendar[['date', 'event_name_1']]\n",
    "                .dropna()\n",
    "                .reset_index(drop=True)\n",
    "                .rename(columns={'date': 'ds', 'event_name_1': 'holiday'}))\n",
    "holidays[\"lower_window\"] = -4\n",
    "holidays[\"upper_window\"] = 3\n",
    "\n",
    "forecasts_dict = forecast_hts(y=sales_long, h=28, nodes=nodes, holidays=holidays, method = \"FP\", daily_seasonality=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_hts(y=forecasts_dict, skipFitting=True, h=28, nodes=nodes, holidays=holidays, method = \"PHA\", daily_seasonality=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in forecasts_dict.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### forecast with pmdarima, reconcile with htsprophet\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\tilde{y}_h = S G \\hat{y}_h $$, where $G$ is the &rsquo;reconciliation&rsquo; matrix\n",
    "which minimizes the forecast errors of the base forecasts\n",
    "\n",
    "Reconciliation approaches:\n",
    "\n",
    "-   Bottom up (BU)\n",
    "-   Top down:\n",
    "    -   Average historical proportions (AHP)\n",
    "    -   Proportions of historical averages (PHA)\n",
    "    -   Forecast proportions (FP)\n",
    "-   Middle-out\n",
    "-   Optimal (minT):\n",
    "    -   WLS, variance scaling\n",
    "    -   WLS, structural scaling\n",
    "    -   OLS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconciled_forecast = forecast_hts(y=y_pred, skipFitting=True, h=28, nodes=nodes, method = \"PHA\", daily_seasonality=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconciliation with MLSE estimator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pierre and Chiara&rsquo;s latest research on reconciliation:\n",
    "[http://www.efc.pwr.edu.pl/assets/EFC19/Modica_EFC19.pdf](http://www.efc.pwr.edu.pl/assets/EFC19/Modica_EFC19.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VAR model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intro to multivariate least squares:\n",
    "[https://data.library.virginia.edu/getting-started-with-multivariate-multiple-regression/](https://data.library.virginia.edu/getting-started-with-multivariate-multiple-regression/)\n",
    "\n",
    "When fitting a linear model on more than one dependent variables, instead of a\n",
    "weight vectore we get a weight matrix, one row for each dependent variable.\n",
    "\n",
    "If we run separate univariate models and one multivariate model we end up with\n",
    "the same coefficients, the only difference is in the variance-covariance matrix\n",
    "of the model coefficients.\n",
    "\n",
    "The covariance matrix is then obtained from $\\frac{1}{T-Q}(Y-\\hat{A}Z) (Y-\\hat{A}Z)'$ , where Q is the number of estimated parameters.\n",
    "\n",
    "    set.seed(123) # Reset random number generator for reasons of reproducability\n",
    "    \n",
    "    # Generate sample\n",
    "    t <- 200 # Number of time series observations\n",
    "    k <- 2 # Number of endogenous variables\n",
    "    p <- 2 # Number of lags\n",
    "    \n",
    "    # Generate coefficient matrices\n",
    "    A.1 <- matrix(c(-.3, .6, -.4, .5), k) # Coefficient matrix of lag 1\n",
    "    A.2 <- matrix(c(-.1, -.2, .1, .05), k) # Coefficient matrix of lag 2\n",
    "    A <- cbind(A.1, A.2) # Companion form of the coefficient matrices\n",
    "    \n",
    "    # Generate series\n",
    "    series <- matrix(0, k, t + 2*p) # Raw series with zeros\n",
    "    for (i in (p + 1):(t + 2*p)){ # Generate series with e ~ N(0,0.5)\n",
    "      series[, i] <- A.1%*%series[, i-1] + A.2%*%series[, i-2] + rnorm(k, 0, .5)\n",
    "    }\n",
    "    \n",
    "    series <- ts(t(series[, -(1:p)])) # Convert to time series format\n",
    "    names <- c(\"V1\", \"V2\") # Rename variables\n",
    "    \n",
    "    plot.ts(series) # Plot the series\n",
    "\n",
    "    library(vars) # Load package\n",
    "    \n",
    "    var.1 <- VAR(series, 2, type = \"none\") # Estimate the model\n",
    "    vcov(var.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "sales_train, sales_val = sales_long.iloc[:-28, :], sales_long.iloc[-28:, :]\n",
    "\n",
    "scaler = scaler.fit(sales_train)\n",
    "sales_train_norm = scaler.transform(sales_train)\n",
    "sales_train_norm = pd.DataFrame(sales_train_norm, index=sales_train.index, columns=sales_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "model = VAR(sales_train)\n",
    "results = model.fit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\mathbf{H}^{\\top}  =\\left[\\mathbf{I}_{\\left(N-N_{L}\\right)} \\quad-\\mathbf{A}\\right] $\n",
    "\n",
    "$$\n",
    "\\mathbf{x}=\\mathbf{S x}^{*} \\Longleftrightarrow \\mathbf{H}^{\\top} \\mathbf{x}=\\mathbf{0}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_matrix = np.dot(sum_matrix, sales_long.iloc[:, -n_bottom_nodes:].T)\n",
    "assert all(x_matrix == sales_long.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_matrix = sum_matrix.iloc[:-n_bottom_nodes, :].values\n",
    "h_matrix = np.vstack((np.identity(a_matrix.shape[0]), -a_matrix.T))\n",
    "\n",
    "assert (np.dot(h_matrix.T, sales_long.T)).all() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconciled estimator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{C}_{k}=\\mathbf{H}\\left(\\mathbf{H}^{\\top} \\mathbf{\\Sigma}_{k} \\mathbf{H}\\right)^{-1} \\mathbf{H}^{\\top} \\mathbf{\\Sigma}_{k}\n",
    "$$\n",
    "\n",
    "$ \\hat{\\boldsymbol{\\Theta}}_{k}^{\\mathrm{MLSE}}=\\hat{\\boldsymbol{\\Theta}}_{k}^{\\mathrm{MLS}}\\left(\\mathbf{I}_{N}-\\mathbf{C}_{k}\\right) $\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_cov = results.sigma_u\n",
    "# z = results.endog_lagged\n",
    "# var_cov_matrix = np.kron(np.linalg.inv(z.T @ z), residuals_cov)\n",
    "# var_cov_matrix.shape\n",
    "theta = results.coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = h_matrix @ np.linalg.inv(h_matrix.T @ residuals_cov @ h_matrix) @ h_matrix.T @ residuals_cov\n",
    "proj_matrix = np.identity(n_nodes) - c_matrix\n",
    "\n",
    "reconciled_theta = theta @ proj_matrix.values\n",
    "results.coefs = reconciled_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = results.forecast(sales_train.values[-2:], 28)\n",
    "y_hat = scaler.inverse_transform(y_hat)\n",
    "y_hat = pd.DataFrame(y_hat, index=sales_val.index, columns=sales_val.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconciliation with fable\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rpy2 magic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(tsibble)\n",
    "library(fable)\n",
    "source('~/git/experiments/src/m5/reconcile.R')\n",
    "# library(future)\n",
    "# plan(multiprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i sales_unp\n",
    "\n",
    "sales_unp$date <- as.Date(sales_unp$date, format = \"%y-%m-%d\")\n",
    "\n",
    "sales_ts <- sales_unp %>%\n",
    "    as_tsibble(index=date, key = c(item_id, dept_id, cat_id, store_id, state_id))\n",
    "\n",
    "sales_agg <- sales_ts %>%\n",
    "    group_by(state_id) %%\n",
    "        summarise(sales = sum(sales))\n",
    "sales_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    aggregate_sales <- function(dated_sales){\n",
    "      dated_sales$date <- as.Date(dated_sales$date, format = \"%y-%m-%d\")\n",
    "    \n",
    "      sales_ts <- dated_sales %>%\n",
    "        as_tsibble(index=date, key = c(item_id, dept_id, cat_id, store_id, state_id))\n",
    "        ## fill_gaps(.full = TRUE)\n",
    "    \n",
    "      sales_agg <- sales_ts %>%\n",
    "        aggregate_key((state_id / store_id / dept_id) * item_id, sales = sum(sales))\n",
    "    }\n",
    "    \n",
    "    fit_model <- function(sales_agg, model_spec){\n",
    "      ## model_spec <- ETS(sales ~ error(\"A\") + trend(\"N\") + season(\"N\"), opt_crit = \"mse\")\n",
    "      sales_fit <- sales_agg %>%\n",
    "        ## filter_index(\"2016-01-01\" ~ \"2016-03-27\")  %>%\n",
    "        model(mod = model_spec)\n",
    "    }\n",
    "    \n",
    "    forecast_reconciled <- function(sales_fit){\n",
    "      sales_fc <- sales_fit  %>%\n",
    "        reconcile(\n",
    "          ## method = c(\"wls_var\", \"ols\", \"wls_struct\", \"mint_cov\", \"mint_shrink\"),\n",
    "          mod = min_trace(mod, method='mint_shrink')\n",
    "        ) %>%\n",
    "        forecast(h=\"28 days\")\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\*\\*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### jupyter-R\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    library(arrow)\n",
    "    library(tsibble)\n",
    "    library(fable)\n",
    "    library(tidyverse)\n",
    "    \n",
    "    sales_unp <- read_parquet(\"/Users/luca/git/experiments/data/sales_unp.parquet\", as_tibble=TRUE)\n",
    "    sales_unp$date <- as.Date(sales_unp$date, format = \"%y-%m-%d\")\n",
    "    \n",
    "    sales_ts <- sales_unp %>%\n",
    "        as_tsibble(index=date, key = c(item_id, dept_id, cat_id, store_id, state_id))\n",
    "\n",
    "    sales_agg <- sales_ts %>%\n",
    "        aggregate_key((state_id / store_id / dept_id) * item_id, sales=sum(sales))\n",
    "    sales_agg\n",
    "\n",
    "    source('~/git/experiments/src/m5/reconcile.R')\n",
    "    \n",
    "    sales_agg <- sales_ts %>% aggregate_sales()\n",
    "    \n",
    "    model_spec <- ETS(sales ~ error(\"A\") + trend(\"N\") + season(\"N\"), opt_crit = \"mse\")\n",
    "    sales_fit <- sales_agg %>% fit_model(model_spec)\n",
    "    \n",
    "    sales_fc <- sales_fit %>% forecast_reconciled()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submit\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [[https://github.com/Kaggle/kaggle-api][Kaggle API]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Credentials\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by exporting `KAGGLE_USERNAME` and `KAGGLE_KEY` in `.envrc`. We can\n",
    "find the credentials on kaggle.com after logging in.\n",
    "\n",
    "We then install the kaggle API with `poetry add kaggle`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    cd ~/git/experiments/ &&\n",
    "        poetry run kaggle competitions download -c m5-forecasting-accuracy -p data/\n",
    "\n",
    "    unzip ~/git/experiments/data/m5-forecasting-accuracy.zip -d ~/git/experiments/data/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission format\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(data / 'sample_submission.csv')\n",
    "sample_submission.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first half of the submission concerns the validation set, for days\n",
    "1914-1941.\n",
    "The second half concerns the test set, days 1942-1969. These will only be\n",
    "evaluated in the last month so we just fill them with zeros.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dummy submission\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First dummy submission: just use the last 28 days in the train data!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data / 'sales_train_validation.csv')\n",
    "\n",
    "submission = pd.read_csv(data / 'sample_submission.csv')\n",
    "\n",
    "n_val_rows = int(submission.shape[0] / 2)\n",
    "\n",
    "n_forecasted_dates = 28\n",
    "\n",
    "submission.iloc[:n_val_rows, -n_forecasted_dates:] = df.iloc[:, -n_forecasted_dates:].values\n",
    "submission.iloc[-n_val_rows:, -n_forecasted_dates:] = 1000\n",
    "\n",
    "submission.to_csv(data / 'submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write submission file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(data / 'sample_submission.csv')\n",
    "\n",
    "n_val_rows = int(submission.shape[0] / 2)\n",
    "n_forecasted_dates = 28\n",
    "\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.iloc[:n_val_rows, :] = df.iloc[:, -n_forecasted_dates:].values\n",
    "\n",
    "submission.to_csv(data / 'submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit and see leaderboard\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    cd ~/git/experiments/\n",
    "    poetry run kaggle competitions submit -c m5-forecasting-accuracy -f data/submission.csv -m \"Dummy submission\"\n",
    "\n",
    "    kaggle competitions submissions m5-forecasting-accuracy\n",
    "\n",
    "fileName        date                 description       status    publicScore  privateScore\n",
    "---------&#x2013;&#x2014;  --------------&#x2013;&#x2014;  -----------&#x2013;&#x2014;  ---&#x2013;&#x2014;  ------&#x2013;&#x2014;  -------&#x2013;&#x2014;\n",
    "submission.csv  2020-03-06 12:27:04  Dummy submission  complete  1.15563      None\n",
    "\n",
    "fileName        date                 description       status    publicScore  privateScore\n",
    "---------&#x2013;&#x2014;  --------------&#x2013;&#x2014;  -----------&#x2013;&#x2014;  ---&#x2013;&#x2014;  ------&#x2013;&#x2014;  -------&#x2013;&#x2014;\n",
    "submission.csv  2020-03-06 12:27:04  Dummy submission  complete  1.15563      None\n",
    "\n",
    "    kaggle competitions leaderboard m5-forecasting-accuracy -s\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interesting packages\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   AutoTS, support for Croston and hierarchical TS: [https://github.com/antoinecarme/pyaf](https://github.com/antoinecarme/pyaf)\n",
    "    -   N-BEATS (winner of M4): [https://github.com/takotab/fastseq](https://github.com/takotab/fastseq)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "org": null,
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
